{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick note: this workshop is designed to follow the ISLR textbook. We recommend that you have a very basic understanding of python before running this, which you could probably get for free in a few hours from DataCamp, Udemy, EdX or Coursera. As a result this course will not cover the basics of python (such as data types) - please ask us if there are elements of the course which are not clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Python for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start by setting up our analytical environment. Since python is modular this means that we will start by adding some additional tools that will make our lives easier.\n",
    "\n",
    "Pandas is probably the most popular and versatile data analysis tool in python. We'll be doing almost all of our work using pandas, so we will import it with the \"import\" statement. Pandas relies on another package called \"numpy\", so we will also need to import that.\n",
    "\n",
    "Numpy: http://www.numpy.org/  \n",
    "Pandas: https://pandas.pydata.org/\n",
    "\n",
    "Notice that we are giving the packages nicknames using \"as\" so that we can use them more easily later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up dataset in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is setting up the dataset that we're going to use for this workshop. Luckily the \"boston\" dataset that we want to use is included as part of the \"sklearn\" package. We need to import a specific module called \"datasets\".\n",
    "\n",
    "Note how this is a different syntax from the import statements above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use one of the methods of the datasets module to import the boston dataset - we won't explain this in more detail because it's not particularly generalisable and i/o methods are outside the scope of this lab. \n",
    "\n",
    "Note that the dot notation means that we're digging down in tho the hierarchy of the module to call a particular method. Most modules are arranged so that similar methods are clustered together into one category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_in = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic data inspection in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boston data that we imported has two separate components, the data and the target, essentially the suggested independent and dependent variables. We're going to start by constructing one complete dataset, but we don't really need to do that for a good proportion of the analysis we will run - many analytical functions in python make us specify the dependent and independent variables as separate inputs.\n",
    "\n",
    "Below we have demonstrated both creating a pandas DataFrame and adding a column to it.\n",
    "\n",
    "Notice that to create a new pandas DataFrame we actually call a method from the pandas api. We pass arguments indicating the name of input data and the names of the columns (which were not included in our input dataset).\n",
    "\n",
    "Adding a new column is simply a matter of specifying a new column name and reading in the data on the right hand side of the statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_pd = pd.DataFrame(boston_in.data,columns = ['crim','zn','indus','chas','nox','rm','age','dis','rad','tax','ptratio','black','lstat'])\n",
    "boston_pd['medv'] = boston_in.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually inspect the boston dataset in python in several ways.\n",
    "\n",
    "First, we can look at the variable names using list() - in general the function of list is to convert an \"iterable\" into a list, but it gives the result we want here. We could also achieve this with \"boston_pd.columns.tolist()\", but there is no benefit to the extra typing! This is an example of the hierarchical dot notation discussed above - we're digging down to the column related methods using \".column\" and then specifically calling \".tolist()\".\n",
    "\n",
    "Note that we've wrapped the whole thing in a print function. This isn't strictly necessary in this case as python usually outputs the results of the last comman - we could simply have run \"*list(boston_pd)*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'black', 'lstat', 'medv']\n"
     ]
    }
   ],
   "source": [
    "print(list(boston_pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second approach, we select the first 6 rows using the square brackets notation after the name of the DataFrame.\n",
    "\n",
    "Notice that python's indexing starts at zero, and also that this statement returns everything up to but not including row at index 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad    tax  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "5  0.02985   0.0   2.18   0.0  0.458  6.430  58.7  6.0622  3.0  222.0   \n",
       "\n",
       "   ptratio   black  lstat  medv  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  \n",
       "5     18.7  394.12   5.21  28.7  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_pd[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the third approach, we use a built-in method of the DataFrame in order to display the first five rows. Notice that the \"head\" method ends with open brackets but doesn't take any arguments - that's quite normal for python. The brackets tell python to call the method rather than reference it; it's the difference between asking for someone to make we dinner or asking them to give we the recipe for dinner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad    tax  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   ptratio   black  lstat  medv  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a simple linear model we're going to need to import the tools that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a basic linear model is easy in python. During this workshop we will explore more than one way to create a linear model in python, but we are going to start with the one that is most similar to R - the statsmodels formula api. This implements patsy, which is a python version of the formula language used by R.\n",
    "\n",
    "On the whole this is probably not the best approach because patsy is not a perfect read across from the R language, and it makes building a simple linear regression look fairly different to building more complex models that we will cover later.\n",
    "\n",
    "The code below shows clearly that there are actually two steps to building a basic linear model: defining the model and fitting the model. There is actually no need to split these two steps from a coding perspective - we can simply append the \".fit()\" to the first line. For simplicity we will combine those steps from now on.\n",
    "\n",
    "Note that we are calling the \".ols()\" method (\"ordinary least squares\"), and providing both a definition of the model we want to fit (\"explain variation in medv using lstat\") and the name of the DataFrame where the variables are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmo = smf.ols('medv ~ lstat' ,data=boston_pd)\n",
    "lm = lmo.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to display a basic summary of our linear model using the \"summary\" method of our linear model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the statistics above (and more) are available to inspect on an individual basis though the linear model object that we created. We will examine a small number of these below.\n",
    "\n",
    "First, we will examine the model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we will look at the confidence intervals of the model coefficients. I have selected a 95% confidence interval.\n",
    "\n",
    "Note that the output of this is actually a pandas DataFrame, whilst the model coefficients were outputs as a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.conf_int(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thirdly, we can examine the significance values of individual coefficients.\n",
    "\n",
    "Note that we have used label indexing to select the t-statistic and p-values for the lstat coefficient in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The t-value of the lstat coefficient is %0.2f, and the associated p-value is %0.2E\") % (lm.tvalues['lstat'], lm.pvalues['lstat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourthly, we can examine the overall statistical significance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The F-Statistic of the model is %0.2f and associated p-value is %0.2E\") % (lm.fvalue, lm.f_pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifthly, we can examine the measures of fit, such as R-squared and R-squared adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The R-squared value is %0.3f and the R-squared adjusted is %0.3f\") % (lm.rsquared, lm.rsquared_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we want to examine the rest of the methods available to we, we can use \"dir()\" to examine the methods associated with our linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting new observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to use python to generate predicted observations with the methods of our linear model object. For this exercise we will generate some artificial data for the percentage of lower status population in the area (lstat) to predict median house value (medv), but of course for a real application we are more likely to observe rather than generate a new value for lstat.\n",
    "\n",
    "The first step is to create the new \"observations\". Note that we are specifying the column name; this is not just for aesthetics - the prediction method won't work if we won't specify the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xnew = pd.DataFrame([5,10,15],columns = ['lstat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model object that we created has a \"predict\" method which can be used to generate the new  values that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ynewpred =  lm.predict(Xnew)\n",
    "print(ynewpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the latest version of statsmodels there isn't a simple implementation of confidence and prediction intervals for predicted values - as a result we have to calculate them from scratch. \n",
    "\n",
    "It's outside the scope of this workshop to discuss exactly how this code works, although it isn't particularly complex maths or code. Instead, we will highlight a few features that are of interest:\n",
    "\n",
    "**Firstly**, that double-asterisk notation used to raise one number to the power of the other, including to generate a square root.  \n",
    "\n",
    "**Secondly**, that we can use \".sum()\" to get the total of a Series data type.  \n",
    "\n",
    "**Thirdly**, that our linear model has methods to show both the number of observations (lm.nobs) and the means squared error of the residuals (lm.mse_resid).  \n",
    "\n",
    "**Fourthly**, that Xnew is a pandas Series, not an numpy array - that means that we cannot just use simple mathematical operators to subtract the values in an numpy array from it even if they are the same dimensions. Below we have written two different solutions to this problem. One of them converts the values in the Series into a numpy array using the \".values\" method (outputting an numpy array), whilst the other uses the \".sub()\" method of the pandas Series, which can cope with a number of other element-wise data types (outputting a pandas Series).   \n",
    "\n",
    "In this code there is no difference in the output for either solution, but a pandas Series is a more complex data type with more methods and it is probably better to pick the data type that suits our application and stick with it rather than constantly making arbitrary conversions.  \n",
    "\n",
    "If we want to explore the functional differences between a pandas Series and a numpy array, we can use the dir() function (as demonstrated above). For example, we could run \"print(dir(np.ndarray))\" and \"print(dir(pd.Series))\".\n",
    "\n",
    "we can use the \"type()\" method to return the data type of any object that we pass as an argument, if we want to explore either the different input and output data types. For example, we can run \"print(type(Xnew['lstat']))\" and \"print(type(Xnew['lstat']).values)\" to understand the way the different solutions change the input data types. By running one solution instead of the other and running \"print(type(rpr_sq))\" at the end, we can see that the different solutions result in different outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "#                     /-------------------\n",
    "# t_stat * std_er *  /            rpr_sq\n",
    "#                   /  inv_ob + ----------\n",
    "#                  v               sxx\n",
    "\n",
    "# ssx = sm_x2s - sm_xs2\n",
    "#                -------\n",
    "#                lm.nobs\n",
    "\n",
    "# Sum of the x^2's \n",
    "sm_x2s = ((boston_pd['lstat']**2).sum())\n",
    "\n",
    "# Sum of the x's squared\n",
    "sm_xs2 = (boston_pd['lstat'].sum())**2\n",
    "\n",
    "# Calculating Sxx\n",
    "sxx = (sm_x2s - sm_xs2/lm.nobs)\n",
    "\n",
    "# T statistic\n",
    "t_stat = stats.t.ppf(1-0.025, lm.nobs - 2)\n",
    "\n",
    "# Standard error (square root of MSE) [tick]\n",
    "std_er = lm.mse_resid**0.5\n",
    "\n",
    "# Inverse of the nubmer of observatinos\n",
    "inv_ob = (1/lm.nobs)\n",
    "\n",
    "# Residual of the predicted value squared  [mean medv: fick]\n",
    "rpr_sq = (Xnew['lstat'].sub((boston_pd['lstat'].mean())))**2\n",
    "\n",
    "# This is an alternative construction of the line above which converts the Series into an array\n",
    "#rpr_sq = (Xnew['lstat'].values - (boston_pd['lstat'].mean()))**2\n",
    "\n",
    "# Calculating confidence interval/2\n",
    "ci = t_stat*std_er*((inv_ob  + rpr_sq/sxx)**0.5)\n",
    "\n",
    "# Calculating prediction interval/2\n",
    "pi = t_stat*std_er*((1+inv_ob  + rpr_sq/sxx)**0.5)\n",
    "\n",
    "# Recreating confidence interval matrix\n",
    "ci_pd = pd.DataFrame(ynewpred,columns=['fit'])\n",
    "ci_pd['lwr'] = ynewpred-ci\n",
    "ci_pd['upr'] = ynewpred+ci\n",
    "\n",
    "# Recreating prediction interval matrix\n",
    "pi_pd = pd.DataFrame(ynewpred,columns=['fit'])\n",
    "pi_pd['lwr'] = ynewpred-pi\n",
    "pi_pd['upr'] = ynewpred+pi\n",
    "\n",
    "print(ci_pd)\n",
    "print(pi_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising data and diagnostic plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to visualise our data in python by importing the popular package \"matplotlib\". It's worth noting that there are \"competitors\" to matplotlib which make some of the tasks below much easier.\n",
    "\n",
    "**statsmodels** has it's own visualisation module that is built on top of matplotlib, but the appearance of those graphs is less easy to control and using it requires a complete re-write of the code.\n",
    "\n",
    "**seaborn** is a very popular (and more modern) visualisation package for python which is also built on top of matplotlib - we don't explore it here only because it is roughly equivalent for the purposes of this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, it's not a very good idea to just run a regression model without visually inspecting our data. So we will construct a plot comparing lstat with medv. matplotlib works by adding elements to our plot one at a time. This way we can construct very complex and interesting plots from relatively simple components.\n",
    "\n",
    "we can see below that we start by creating a scatter plot of the two variables (the x-axis variable is the first argument, the y-axis variable is the second argument), then adding a label to the x and y axis. \n",
    "\n",
    "We put all these components together and display them using the \".show()\" method of pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(boston_pd['lstat'],boston_pd['medv'])\n",
    "plt.xlabel('lstat', fontsize=12)\n",
    "plt.ylabel('medv', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above suggests that our initial naive approach regressing lstat against medv might have been mistaken as the relationship doesn't appear to be linear, but we can show that more clearly by adding our fitted values to the graph. Compare the additional elements of the code below compared to the above.  \n",
    "\n",
    "**Firstly**, we have specified the colour and shape of the markers for the scatter graph using optional arguments of the \".scatter\" method. Most components of a plot can be customised in the same way - we can see that we have used the \"fontsize\" argument to change the size of the axis labels too. \n",
    "\n",
    "**Secondly**, we have added the trend line of our linear model using the \".plot\" method, also specifying the colour and linewidth using optional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(boston_pd['lstat'],boston_pd['medv'],marker=\"+\",c=\"r\")\n",
    "plt.plot(boston_pd['lstat'],lm.fittedvalues, color='blue', linewidth=3)\n",
    "plt.xlabel('lstat', fontsize=12)\n",
    "plt.ylabel('medv', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to generate some diagnostic graphs to explore the quality of the model that we have fitted. I've added some complex elements to these graphs to aid interpretation (such as a non-parametric trend line to show any pattern in the residuals more clearly), but of course we don't need those elements to produce these diagnostic graphs.\n",
    "\n",
    "In the general course of producing analysis we might run the next few sections of code as one single block, but we have separated them out to make the explanations clearer. One side effect of this is that each code block outputs the memory location of the most recently created object - nothing to worry about.\n",
    "\n",
    "The first step is to define the size of the plot we are using, which ensures that the output is a good resolution for adding to reports. The input for figsize argument is a tuple (denoted by the brackets) representing the horizontal and vertical size of the plot in inches. we could also achieve this using the \"dpi\" argument.\n",
    "\n",
    "Each of the blocks of code starts with the same call to the \".subplot\" method of matplotlib. This allows us to plot more than one graph on a single figure, by changing the three numbers that we pass as an argument. The syntax for a simple arrangement is \"number of horizontal axis\", \"number of vertical axis\", \"position to plot this graph in\". In simple terms, \"221\" means \"this figure will have four plots in it, and this is the first one I want to plot\".\n",
    "\n",
    "Note that we have to repeat the dimensions of the figure for each subplot we want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **first plot** that we will produce is a comparison of the fitted values (using the \".fittedvalues\" method) to the residuals (using the \".resid\" method). In general there should not be a pattern in the residuals. We already have some concerns in this area from the plots that we generated above, because it looks like the relationship between lstat and medv is not linear. As above, notice that the \".subplot\" method is used to specify that this figure will contain four plots and this will be the first.\n",
    "\n",
    "we can see that we have set the markers to black plusses using optional arguments for the \"scatter\" method.\n",
    "\n",
    "The complex elements I've added to this plot are a non-parametric trend line (the second use of the \"scatter\" method) and an horizontal line at zero on the y-axis (using the \"plot\" method). For a good model, this non-parametric line should be close to the zero axis - but we can see in the final plot that the residuals appear to have a pattern.\n",
    "\n",
    "The main features of interest are:\n",
    "\n",
    "**Firstly**, numpy has \"min\" and \"max\" methods which we can use to extract the lowest and highest values of a numpy array. (see the line using the \"plot\" method).\n",
    "\n",
    "**Secondly**, that the syntax for fitting non-linear models is essentially the same as for fitting linear models. By drilling down into the hierarchy of the statsmodels package we can access the general class of non-parametric methods, and then select the lowess regression method: \"sm.nonparametric.lowess\". It's beyond the scope of this module to explain lowess regression, but it will be covered in later workshops. \n",
    "\n",
    "We specify \"return_sorted=False\", because this returns only the fitted values of the non-parametric model as a one dimensional array. By default the \".lowess\" method returns a two dimensional array of both the input dependent variable values and the output fitted values. The \".scatter\" method expects two single dimensional arrays as arguments, so failing to set this option correctly would cause a failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Left\n",
    "plt.subplot(221)\n",
    "plt.scatter(lm.fittedvalues,lm.resid,marker=\"+\",c=\"black\")\n",
    "plt.scatter(lm.fittedvalues,sm.nonparametric.lowess(lm.resid,lm.fittedvalues, return_sorted=False), marker=\"+\", c='red')\n",
    "plt.plot([np.min(lm.fittedvalues),np.max(lm.fittedvalues)],[0,0], color='grey', linewidth=1)\n",
    "plt.xlabel('Fitted values', fontsize=12)\n",
    "plt.ylabel('Residuals', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **second plot** that we will produce is a normal Q-Q plot, which compares the distribution of standardised errors to a theoretical distribution of errors if they were exactly normally distributed. In an ideal world the points plotted on this graph would be as close to the x=y line as possible - the fact that the errors are not normally distributed violates the assumptions of an ordinary least squares model. The exact mathematics behind this and the operation of the code is far beyond the scope of this course. \n",
    "\n",
    "As above, notice that the subplot method is used consistently to indicate that this is a figure with four plots, and this is the second plot.\n",
    "\n",
    "One additional element of this plot is the x=y trend line (an important aid for interpreting the Q-Q plot). Unfortunately this otherwise simple operation is relatively obscured by the complexity of the inputs.\n",
    "\n",
    "Note that using the built in statsmodels visualisation package, this can be plotted using: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Right\n",
    "theoretical_distribution = np.sort((stats.norm.ppf((boston_pd['medv'].index.values + 1)/(lm.nobs+1))))\n",
    "observed_distribution = np.sort(lm.resid_pearson)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.scatter(theoretical_distribution,observed_distribution,marker=\"+\",c=\"black\")\n",
    "plt.plot([min(np.min(observed_distribution),np.min(theoretical_distribution)),max(np.max(observed_distribution),np.max(theoretical_distribution))],[min(np.min(observed_distribution),np.min(theoretical_distribution)),max(np.max(observed_distribution),np.max(theoretical_distribution))], color='grey', linewidth=1)\n",
    "plt.xlabel('Theoretical Quanties', fontsize=12)\n",
    "plt.ylabel('Standardised Residuals', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **third plot** is a comparison between the fitted values and the square root of the standardised residuals (accessed using the \".resid_pearson\" method). This plot is intended to reveal if is there is a relationship between the variance in the error and the magnitude of the fitted values (heteroskedasticity), which violates the assumption of an ordinary least squares model. we can see that as described above, the square root is taken by using the double asterisk notation, indicating raising the number of the power of 0.5 which is equal  to a square root. \n",
    "\n",
    "In this case the arguments in \"subplot\" method indicate were are plotting the third plot. we can see from this that the \".subplot\" method generates plots row-wise before a new line is started.\n",
    "\n",
    "Notice that for this plot we have deliberately changed the part of extent of the plot area using the \".ylim\" method. This allows us to specify the lower and upper limits of the y axis.\n",
    "\n",
    "we can see that we have also added a non-parametric trend line using the same method as before, showing a clear relationship between the magnitude of the fitted values and the variance in the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom Left\n",
    "plt.subplot(223)\n",
    "plt.ylim(0.0,2.0)\n",
    "plt.scatter(lm.fittedvalues,lm.resid_pearson**.5,marker=\"+\",c=\"black\")\n",
    "plt.scatter(lm.fittedvalues,sm.nonparametric.lowess(lm.resid_pearson**.5,lm.fittedvalues, return_sorted=False), marker=\"+\", c='red')\n",
    "plt.xlabel('Fitted values', fontsize=12)\n",
    "plt.ylabel('Square Root Standardised Residuals', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **fourth plot** is a comparison between the standardised residuals and the leverage of each observation. Leverage is a measure of how much an individual observation is impacting the final model that we have build - values with high leverage could be removed to increase the predictive accuracy of the model.  Different measures of leverage are accessed using the \".get_influence()\" method of our linear model object. In particular we and then calling \".hat_matrix_diag\" method, which returns an array corresponding to the diagonal elements of the hat matrix. Potentially concerning observations are located in the top and bottom right of the plot. Explaining why this is a good measure of influence and how it is calculated as out of scope for this workshop. \n",
    "\n",
    "Notice that for this plot we have deliberately changed the part of extent of the plot area using the \".xlim\" method. This allows us to specify the lower and upper limits of the x axis.\n",
    "\n",
    "we can see that we have also added a non-parametric trend line (the second use the \".scatter()\" method) and a y = 0 line (the \".plot()\" method), using the same method as before. In this case there are some high leverage observations, but none that are particularly concerning.\n",
    "\n",
    "The y = 0 line is plotted by selecting the lowest and highest x-axis values (using numpy's \".min()\" and \".max()\" methods as described above), and setting the y-axis values to [0,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom Right\n",
    "plt.subplot(224)\n",
    "plt.xlim(0.00,0.03)\n",
    "plt.scatter((lm.get_influence().hat_matrix_diag),lm.resid_pearson,marker=\"+\",c=\"black\")\n",
    "plt.scatter((lm.get_influence().hat_matrix_diag),sm.nonparametric.lowess(lm.resid_pearson,(lm.get_influence().hat_matrix_diag), return_sorted=False), marker=\"+\", c='red')\n",
    "plt.plot([np.min((lm.get_influence().hat_matrix_diag)),np.max((lm.get_influence().hat_matrix_diag))],[0,0], color='grey', linewidth=1)\n",
    "plt.xlabel('Leverage', fontsize=12)\n",
    "plt.ylabel('Standardised Residuals', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, these graphs are plotted using the \".show()\" method of our plot object. As discussed above, all of these diagnostic plots suggest that we should refit this model taking account of the non-linearity of the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect diagnostic measures for individual elements. \n",
    "\n",
    "In the step below, we use the \".argmax()\" method to get the index of the highest leverage observation. Note that the observations are by default ordered the same in the hat matrix as they are in the matrix of observations. \".argmax()\" is a method for both numpy array's and panda Series, which outputs the index of the observation with the highest value. Be aware that if there are observations tied at the maximum value then \".argmax()\" will return the index of just the first instance of the highest value.\n",
    "\n",
    "Note the simple implementation of the string formatter (\"%s\") to output the result in a sentence. This works by replacing the %s with the actual value provided on the right side of the print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_lev = (lm.get_influence().hat_matrix_diag).argmax()\n",
    "print(\"Index of highest leverage observation: %s\") % high_lev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the index of the highest leverage observation to extract the lstat and medv values of that observation. \n",
    "\n",
    "We use the \".loc()\" method of the pandas DataFrame to select the value in the row and column that we want by using the index of the highest observation (determined above) to select the row we want and the label of the column to select the column we want. Note that \".loc()\" can be used to \"slice\" pandas DataFrames as well as select individual values, we could equally have used \"boston_pd.loc[high_lev,['lstat','medv']]\" to select and output both values as a numpy array.\n",
    "\n",
    "Note the slightly more complex implementation of the string formatter. This time we are using \"%0.2f\" to indicate that the output strings should be floating point integers with two decimal places, and passing two values on the right side of the print function. These values are passed in the order that we want to use them in the statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_lev_lst = boston_pd.loc[high_lev,['lstat']]\n",
    "high_lev_med = boston_pd.loc[high_lev,['medv']]\n",
    "print(\"Vales of highest leverage observation: %0.2f lstat, %0.2f medv\") % (high_lev_lst,high_lev_med)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the index of the highest leverage observation to extract the actual leverage value of that observation.\n",
    "\n",
    "Since the output of the \".get_influence().hat_matrix_diag\" method is an numpy array, we only need use the square brackets notation and the previously determined index to select the relevant observation.\n",
    "\n",
    "For readability, we have enclosed the call to \"lm.get_influence().hat_matrix_diag\" in brackets - we think this makes it clearer that the square brackets are indexing the output, but the code will execute equally well without the brackets.\n",
    "\n",
    "Note that the string formatter allows us to easily control the number of decimal places in the output. In this case \"%0.4f\" outputs a floating point integer with four decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_max = (lm.get_influence().hat_matrix_diag)[high_lev]\n",
    "print(\"Leverage of highest leverage observation: %0.4f \") % lev_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple linear regression is just as easy to implement as simple linear regression in python.\n",
    "\n",
    "As discussed at the start of this workshop, it isn't necessary to put all of our data into one single dataset containing both the dependent and independent variables in order to do analysis in python - we're going to put that into practice now.\n",
    "\n",
    "In this case, we still need to convert the data into a pandas DataFrame as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_DV = pd.DataFrame(boston_in.data,columns = ['crim','zn','indus','chas','nox','rm','age','dis','rad','tax','ptratio','black','lstat'])\n",
    "boston_IV = pd.DataFrame(boston_in.target,columns = ['medv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to build our multiple linear regression model using the regular statsmodels api (we used the statsmodels formula api above). Below are the main differences between this and the way way fitted our first model:\n",
    "\n",
    "**Firstly**, the \".OLS()\" method takes different arguments. Rather than the analysis dataset and formula, the \".OLS()\" method called from the statsmodels api requires the independent and dependent variables as arguments.\n",
    "\n",
    "**Secondly**, it's important to notice that the \".OLS()\" method does not automatically add a constant - we need to use the \".add_constant()\" method in order to do that. In particular, it's important to see that the \".add_constant()\" method is called directly from the statsmodels api rather than from the pandas dataset (which is instead taken as an argument).\n",
    "\n",
    "**Thirdly**, when we defined and fitted our simple linear regression above, we did it in two steps for additional clarity. For brevity, we have defined and fitted the multiple linear regression in one single line. In either case, both approaches are valid and work in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = sm.OLS(boston_IV,sm.add_constant(boston_DV)).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the simple linear regression, we can use the \".summary()\" method of our linear model object to explore the quality of the model. The results are shown below\n",
    "\n",
    "we can see below that this model has fairly good explanatory power (R-squared = 0.741), but that the \"kitchen sink\" approach of including all possible dependent variables is not without drawbacks. \n",
    "\n",
    "**Firstly**, some of our dependent variables are not really worth including (\"indus\" and \"age\" are not statistically significant - see P>|t| column). \n",
    "\n",
    "**Secondly**, the explanatory power has come at a cost of parsimony (fitting the simplest good model possible) - we can tell this because the Akaike information criterion (AIC) and Bayesian information criterion (BIC) scores are actually lower for this model than for the simple linear model! It is beyond the scope of this workshop to explain AIC and BIC, but in general we want to maximise these measures of parsimony whilst maintaining as much explanatory power as possible.\n",
    "\n",
    "**Thirdly**, this is slightly different to the output that you would get if you ran this analysis identically in R. The reason for that, as far as we can tell, is that there are very slightly differences in the way R and python handle datasets with multicollinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore a more detailed measure of parsimony - variance inflation factor (VIF). \n",
    "\n",
    "When we fit two highly correlated variables in the same model, this can greatly increase the variability in the model coefficient estimates. VIF is a measure of how much of the variability in a coefficient is due to collinearity (that is, correlation between dependent variables). In general we should remove variables that have a high VIF.\n",
    "\n",
    "The first thing we need to do is import specific diagnostic measure we want from the statsmodels package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VIF is calculated on a variable by variable basis - in the sense that it is calculated by taking one variable out at a time and looking at the effect on the variability of the coefficient estimates. Of course that means that VIFs are only valid in the context of the other variables - if we decide to remove a variable from our model then we will need to recalculate VIFs for the remaining variables.\n",
    "\n",
    "This means that we need to manually construct a table with one line per variable.\n",
    "\n",
    "The first step is to set up an empty pandas DataFrame and add a column with one line per variable. Crucially this needs to include the constant as the VIFs are different with and without the constant included! Notice that extracting the column names from a DataFrame is as easy as calling the \".columns\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "vif[\"Features\"] = sm.add_constant(boston_DV).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to calculate our VIFs and put them into a second column.\n",
    "\n",
    "The \".variance_inflation_factor()\" method takes two arguments - the dataset of the variables we want to test and the index of the variable that we want to calculate a VIF for. In order to generate a table of VIFs we need to run this code once per variable. The main features of the code are explained below:\n",
    "\n",
    "**Firstly**, the whole statement is enclosed in square brackets, which means that the output is a \"list\".\n",
    "\n",
    "**Secondly**, we've used the \".variance_inflation_factor()\" method as described above, but instead of specifying a specific index we have added the variable \"i\" as the second argument. This allows us to automate the calculation of a VIF for each variable by changing the value of *i*.\n",
    "\n",
    "**Thirdly**, it's the *for* keyword which links the function we defined on the left to the changing value of *i* on the right.\n",
    "\n",
    "**Fourthly**, we change the value of *i* using *in [list of values]*. In this case the list of values is generated using the \"range()\" function (described below), but in general the impact of this statement is to run the function we defined using each value of *i* provided in the list.\n",
    "\n",
    "**Fifthly**, we use the \"range()\" function to generate a list of values of *i*.  The \"range()\" function is  flexible in the way that it operates depending on how many arguments we pass, but we are only passing one argument here. In that case, the \"range()\" function creates a list of values from 0 up to but not including the integer provided as an argument (because python uses zero based indexing). For example, range(3) would generate the list: [0,1,2].\n",
    "\n",
    "**Sixthly**, we determine the value to pass to the range function based on the how many variables there are in the model. The \"len()\" function returns the length of a data structure - in this case we use the column of features that we have created as the length of this column is equal to the number of variables by definition! Note that because this returns the number of variables (which is one more than the index value of the final variable) this can be passed directly to the \"range()\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif[\"VIF Factor\"] = [variance_inflation_factor(sm.add_constant(boston_DV).values, i) for i in range(len(vif[\"Features\"]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print return the results below. Removing variables with a high VIF is an art rather than a science, but many textbooks suggest that a value of 5 is concerning, and a value of 10 is very concerning. We can see in the table below that we have several variables that meet this threshold, suggesting that we should be more discriminating in our variable selection.\n",
    "\n",
    "As above, these VIFs will be slightly different to those produced by R due to the slight differences in calculation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, it seems logic that we should at least drop the age variable and refit the model. The good news is that we don't have to completely reconstruct our dependent variable dataset, but can instead drop the age variable using the \".drop()\" method of our pandas DataFrame.\n",
    "\n",
    "The \".drop()\" method has two arguments: the column to be dropped and whether we are indicating the location of that column using an index (\"axis=0\") or label (\"axis=1\").\n",
    "\n",
    "Other than that, the model is build in the same way. We can see from the output that removing this variable has not materially changed the explanatory power of the model (R-squared = 0.741), but the parsimony of the model has very slightly improved (AIC = 3024 and BIC = 3079). Of course the model is still far from ideal, but you will have an opportunity during the lab to explore this with a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3 = sm.OLS(boston_IV,sm.add_constant(boston_DV).drop([\"age\"], axis=1)).fit()\n",
    "lm3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we may want to build the combined impact of dependent variables into a linear model, which are called \"interaction terms\". Interaction terms are calculated by multiplying one variable by another, giving us the ability to understand if one variable changes at different rates based on the value of another variable.\n",
    "\n",
    "In python it is easy to add these terms just by calculating the product of our terms of interest and then including them in the model, so our first step will be to select or create the variables we want to include.\n",
    "\n",
    "Below, we use another method of selecting columns from a pandas DataFrame. The double brackets notation \"*[[\"lstat\",\"crim\"]]*\" is essentially shorthand for the more explicit \".loc()\" method \"*.loc[:, [\"lstat\",\"crim\"]]*\"\n",
    "\n",
    "Note that creating a new column that is the product of two others is as simple as selecting and multiplying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_DV4 = sm.add_constant(boston_DV[[\"lstat\",\"crim\"]])\n",
    "selected_DV4[\"lstat*crim\"] = boston_DV[\"lstat\"]*boston_DV[\"crim\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the linear model as normal below. It's interesting to note that in the case of \"lstat\" (% lower status of the population) and \"crim\" (per capita crime rate by town), there is a small interaction effect. This suggests that there is an underlying relationship between \"lstat\" and \"crim\" (maybe high crime areas are more affordable for the lower status population?) - including an interaction effect makes an adjustment for that.\n",
    "\n",
    "On the whole, this model has very slightly better explanatory power than our original \"lstat\"-only model (R-squared = 0.556 vs 0.544), but other than that the diagnostic statistics are essentially the same. We determined above that one of the main issues with the \"lstat\"-only model was that the relationship with \"medv\" was non-linear - adding more variables and interaction terms was never likely to address that.\n",
    "\n",
    "Note that we could have achieved this equally using the statsmodels formula api: *lm4 = smf.ols('medv ~ lstat * dis' ,data=boston_pd).fit()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm4 = sm.OLS(boston_IV,selected_DV4).fit()\n",
    "lm4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have discussed it several times above, our next step will be to include a non-linear term in our model. We will apply two simple transformations below - square root and natural log. \n",
    "\n",
    "For the square root model we construct the dataset of dependent variables as before, and use the double asterisk notation discussed previously to calculate a square root.\n",
    "\n",
    "This model is a definite improvement over our original attempt: it has better explanatory power (R-sqared is 0.668 vs 0.544) as is more parsimonious (AIV is 3128 vs 3287, BIC is 3141 vs 3295)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_DV5 = sm.add_constant(boston_DV[[\"lstat\"]])\n",
    "selected_DV5[\"sqrt(lstat)\"] = boston_DV[\"lstat\"]**0.5\n",
    "lm5 = sm.OLS(boston_IV,selected_DV5).fit()\n",
    "lm5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the natural log model we construct the dataset of dependent variables as before, but note that we have calculated the natural log using the \".apply()\" method. The \".apply\" method takes a function as an argument and then applies it either row-wise or column-wise to the DataFrame - since the default is column-wise we haven't specified it in our code. It's also important to note that we haven't *called* the math.log method (by adding parenthesis after the method name) but *referenced* it.\n",
    "\n",
    "The natural log method is not available by default, so we have imported the \"math\" package where we can access a wider range of mathematical functions.\n",
    "\n",
    "This model is a definite improvement over our original attempt: it has better explanatory power (R-sqared is 0.674 vs 0.544) as is more parsimonious (AIV is 3119 vs 3287, BIC is 3131 vs 3295). It's also slightly better than the model including a square root term above, but not greatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "selected_DV6 = sm.add_constant(boston_DV[[\"lstat\"]])\n",
    "selected_DV6[\"log(lstat)\"] = (boston_DV[\"lstat\"]).apply(math.log)\n",
    "lm6 = sm.OLS(boston_IV,selected_DV6).fit()\n",
    "lm6.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Model Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in other statistical analysis software, statsmodels has an implementation of ANOVA which allows us to compare nested models. Models are considered nested if one model is a sub-set of the other. For example, if both models include \"lstat\" but one model also includes \"log(lstat)\" the models are nested.On the other hand, if one model contained \"lstat\" and \"log(lstat)\" and the other contained \"lstat\" and \"sqrt(lstat)\" they would not be nested models because neither model is wholly contained in the other. \n",
    "\n",
    "ANOVA allows us to estimate whether the more complex model explains a statistically significant amount of additional variation in the independent variable compared to the simpler model. The exact operation of ANOVA is beyond the scope of this workshop. \n",
    "\n",
    "We need to dig into the hierarchy of the statsmodels api to access the \".anova_lm()\" method which we we use to compare one or more nested models. Note that the order of the models is important: the least complex model is first followed by any other models to compare in order of increasing complexity.\n",
    "\n",
    "We can see from the ANOVA tables below that both non-linear terms are a statistically significant improvement on the model that only includes the linear term. The model that includes \"log(lstat)\" has the smallest amount of remaining unexplained variation (compare \"ssr\"), and is therefore slightly more significantly different when compared to the model which only contains the linear \"lstat\" term (compare \"Pr(>F)\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.anova_lm(lm,lm5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.anova_lm(lm,lm6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand that this model comparison cannot by itself show you which model to select. Below we have used the same ANOVA method to compare the model only including the \"lstat\" term to the model we fitted second including all the dependent variables - this shows that the full model has even less unexplained variation and is therefore even more significantly different from the model only including the \"lstat\" term. This shows how important it is to consider a range of model diagnostics (e.g. AIC, BIC, R-Squared Adjusted) when choosing a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.anova_lm(lm,lm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally (for interest), we will use the same diagnostic graphs as above to understand the quality of the model which includes the \"log(lstat)\" term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Top Left\n",
    "plt.subplot(221)\n",
    "plt.scatter(lm6.fittedvalues,lm6.resid,marker=\"+\",c=\"black\")\n",
    "plt.scatter(lm6.fittedvalues,sm.nonparametric.lowess(lm6.resid,lm6.fittedvalues, return_sorted=False), marker=\"+\", c='red')\n",
    "plt.plot([np.min(lm6.fittedvalues),np.max(lm6.fittedvalues)],[0,0], color='grey', linewidth=1)\n",
    "plt.xlabel('Fitted values', fontsize=12)\n",
    "plt.ylabel('Residuals', fontsize=12)\n",
    "\n",
    "# Top Right\n",
    "theoretical_distribution = np.sort((stats.norm.ppf((boston_pd['medv'].index.values + 1)/(lm6.nobs+1))))\n",
    "observed_distribution = np.sort(lm6.resid_pearson)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.scatter(theoretical_distribution,observed_distribution,marker=\"+\",c=\"black\")\n",
    "plt.plot([min(np.min(observed_distribution),np.min(theoretical_distribution)),max(np.max(observed_distribution),np.max(theoretical_distribution))],[min(np.min(observed_distribution),np.min(theoretical_distribution)),max(np.max(observed_distribution),np.max(theoretical_distribution))], color='grey', linewidth=1)\n",
    "plt.xlabel('Theoretical Quanties', fontsize=12)\n",
    "plt.ylabel('Standardised Residuals', fontsize=12)\n",
    "\n",
    "# Bottom Left\n",
    "plt.subplot(223)\n",
    "plt.ylim(0.0,2.0)\n",
    "plt.scatter(lm6.fittedvalues,lm6.resid_pearson**.5,marker=\"+\",c=\"black\")\n",
    "plt.scatter(lm6.fittedvalues,sm.nonparametric.lowess(lm6.resid_pearson**.5,lm6.fittedvalues, return_sorted=False), marker=\"+\", c='red')\n",
    "plt.xlabel('Fitted values', fontsize=12)\n",
    "plt.ylabel('Square Root Standardised Residuals', fontsize=12)\n",
    "\n",
    "# Bottom Right\n",
    "plt.subplot(224)\n",
    "plt.xlim(0.00,0.03)\n",
    "plt.scatter((lm6.get_influence().hat_matrix_diag),lm6.resid_pearson,marker=\"+\",c=\"black\")\n",
    "plt.scatter((lm6.get_influence().hat_matrix_diag),sm.nonparametric.lowess(lm6.resid_pearson,(lm6.get_influence().hat_matrix_diag), return_sorted=False), marker=\"+\", c='red')\n",
    "plt.plot([np.min((lm6.get_influence().hat_matrix_diag)),np.max((lm6.get_influence().hat_matrix_diag))],[0,0], color='grey', linewidth=1)\n",
    "plt.xlabel('Leverage', fontsize=12)\n",
    "plt.ylabel('Standardised Residuals', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that the non-linear model (including the \"log(lstat)\" term) that we fitted looks much better then the original linear model. Although there clearly some pattern to the residuals (top right) and there is definitely some heteroskedasticity (bottom left), it is much less pronounced than it was before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's simple to create or include qualitative predictors using the statsmodels api. Below we will explore both of these, creating custom functions, and extend our application of the ANOVA comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including Existing Qualitative Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boston dataset already has one qualitative predictor included (\"chas\" - a variable indicating which side of the river the house is on). There are no special steps to include a qualitative predictor in the statsmodels api, so below we take a copy of our previous dependent variable dataset and then add the \"chas\" variable back in.\n",
    "\n",
    "Note that we use the \".copy()\" method to take a copy of the of the DataFrame and columns that we are interested in. The reason that we do this is to make sure we are creating a new object rather than just creating a new reference to an old object - the difference between building a new city and giving a city a second name that people can call it by.\n",
    "\n",
    "For example, the city of Swansea has a Welsh name (Abertawe) - if you increase the population of Swansea by 200 people you wouldn't be surprised that the population of Abertawe also increased by 200. In the code below we want to build a new (identical) city which we can change independently of the original city. Using the \".copy()\" function ensure that this is what we are doing.\n",
    "\n",
    "You can test this using the \"id()\" function, which returns the \"address\" of an object. If the addresses are the same then you have only created a new reference; if they are different then you have created a new object. You can examine the addresses of both objects using this code: \"*print(id(selected_DV7[\"chas\"]),\" \",id(boston_DV[\"chas\"]))*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_DV7 = selected_DV6.copy()\n",
    "selected_DV7[\"chas\"] = boston_DV[\"chas\"].copy()\n",
    "lm7 = sm.OLS(boston_IV,selected_DV7).fit()\n",
    "lm7.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have used an ANOVA comparison to show that the model including the qualitative term explains a statistically significant amount of additional variation compared to the model which does not include the qualitative term. We can also see this by comparing the R-squared values (0.687 vs 0.674). The AIC (3101 vs 3119) and BIC (3118 vs 3131) have continued to decrease, albeit by an increasingly small amount. It may be that we are beginning to hit the point of diminishing returns for adding more variables, or at least that the variables we are adding do not explain a large enough amount of additional variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.anova_lm(lm6,lm7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating New Qualitative Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create a new qualitative predictor. This example will be somewhat artificial by nature, but it will also give us the opportunity to explore creating our own custom functions.\n",
    "\n",
    "We're going to be using the \"rad\" variable because there seem to be two meaningful clusters. Below we've used matplotlib's \".hist()\" method to produced a histogram showing that there is a cluster of houses with less than a rad index of less than 10, and a cluster of houses with a rad index of more than 10. The \".hist()\" method is just as easy to customise as the \".scatter()\" and \".plot()\" methods, but for simplicity we have used the default formatting.\n",
    "\n",
    "Note that, we've calculated the number of bins for our histogram using the \".min()\" and \".max()\" methods of our DataFrame (as described above). Note that we've used the \"int()\" method to cast the result as an integer - this is because we will the \"bins\" argument of matplotlib \".hist()\" method requires an integer rather than a floating point decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = int(boston_DV[\"rad\"].max()-boston_DV[\"rad\"].min())\n",
    "plt.hist(boston_DV[\"rad\"], bins=n_bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have chosen a boundary for our clusters, we are going to create a very simple custom function to categorise our data and apply it to the \"rad\" column of our DataFrame. Here are the main features of the code:\n",
    "\n",
    "**firstly**, we have defined the basic specification of our custom function using the \"def\" keyword. After the \"def\" keyword we need to give the function a name (\"categoricalRadius\"). The capitalisation used here is a convention for user defined functions in python. We also define what arguments the function will expect here. The word \"radius\" inside the brackets lets python know that this function will expect one argument. The \"radius\" variable will therefore be available for use inside our function - it's important to understand that this variable name is arbitrary and could have been almost anything, we chose \"radius\" to make the code easier to understand. Finally, we finish of the declaration with a colon, which lets python know to expect the code of function below.\n",
    "\n",
    "**secondly**, note that the lines of code following the initial declaration are indented. Python is sensitive to indentation and this code will not work without proper indentation. The next time python hits an fully unindented line it will assume that you have finished defining the \"categoricalRadius\" function.\n",
    "\n",
    "**thirdly**, the first line of our custom function (enclosed in triple quotes) is a \"docstring\", which should include an explanation of what the function is meant to do and explain what arguments are expected. Following this convention ensures that every function in python contains its own documentation. You can access the \"docstring\" of our custom function by calling the \".__doc__\" method of the function (e.g. \"*print(categoricalRadius.__doc__)*\"\n",
    "\n",
    "**fourthly**, we have used an \"if else\" statement to process the input data. It's important to see that the statements inside the \"if else\" statement are additionally indented, and that the \"if\" and \"else\" statements both end with a colon, just as our function declaration does. \n",
    "\n",
    "**fifthly**, the \"<=\" notation indicates that we want value of radius to be less than or equal to 10. Python implements all the normal mathematical and logical comparisons you would expect from a programing language. It's interesting to note that although we have defined the comparison using an integer rather than a floating point decimal (\"10\" vs. \"10.0\"), python can handle the fact that our actual data is a floating point decimal.\n",
    "\n",
    "**finally**, in order to produce an output from a custom function we need to use the \"return\" keyword. The \"return\" keyword allows us to output an object from the function for use outside of the function. We have actually been benefiting from this functionality throughout this code (for example: \"*lmo.fit()*\" returns a linear model object that we called \"*lm*\"). In this case we have created the \"catRad\" variable inside the function and that is the object that we want to return. Again, it's important to note that the exact name of the variable we return is arbitrary and has no significant impact on the ouput - variables defined inside a function are only accessible inside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categoricalRadius(radius):\n",
    "    '''Return a categorical variable based on the accessibility to radial highways. Visual inspection reveals accessibilty clusters one where the accessibilty index is less than 10 and one where it is more than 20.'''\n",
    "    \n",
    "    if radius <= 10:\n",
    "        catRad = \"Near\"\n",
    "    else:\n",
    "        catRad = \"Far\"\n",
    "        \n",
    "    return catRad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our custom function, we will construct our dependent variable DataFrame as normal (by copying the our previously created DataFrame), and then add our new variable by using the \".apply()\" method to process the original \"rad\" column. You could use the \".head()\" method to examine the first few rows of our new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_DV8 = selected_DV7.copy()\n",
    "catRad = (boston_DV[\"rad\"]).apply(categoricalRadius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far this example has been slightly artificial - we have deliberately created a qualitative variable in order to turn into dummy variables for our regression model, but it is much more likely that a real analytical project would begin with a qualitative variable and we would then only need to take the steps below to create dummy variables for a model.\n",
    "\n",
    "Below we use the \".get_dummies()\" method to turn our categorical variable into a series of dummy variables. Notice that the \".get_dummies()\" is called directly from the pandas api rather than from the DataFrame we want to add our dummy variables to. The first argument in the \".get_dummies()\" method is the qualitative variable we want to convert into dummy variables, whilst the second specifies the prefix that should be used to identify our new dummy variables. The actual names of the dummy variables are constructed from this prefix, an underscore, and then the various values that the qualitative variable can take (e.g. \"Rad_Far\").\n",
    "\n",
    "In this case the output of \".get_dummies()\" is a DataFrame with two columns, so when we construct the left hand side of the statement we need to make sure that we are initialising two new columns in our dependent variable DataFrame. This is a good example of how flexible python is: we can write statements with multiple or complex outputs and python is able to handle that seamlessly.\n",
    "\n",
    "In the second line of code we are drop one of the newly created dummy variables using \".drop()\" method as described earlier in the workshop. The reason we always drop one dummy variable is because properly constructed dummy variables are multicollinear by definition - in our case if we know that there is a \"1\" for \"Rad_Near\" we can deduce that there is a \"0\" for \"Rad_Far\". This holds true for any number of dummy variables - if we have 4 then we can always deduce the value of the fourth variable if we know the other three. Because of that it is best practice to remove on dummy variable. We could have circumvented this process slightly by setting the optional \"drop_first\" argument of the \".get_dummies()\" method to \"True\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_DV8[[\"Rad_Far\", \"Rad_Near\"]] = pd.get_dummies(catRad, prefix='Rad')\n",
    "selected_DV8 = selected_DV8.drop([\"Rad_Near\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model as usual, it becomes obvious that we have pushed our luck by including more variables. Although the R-squared value has very very slightly increased compared to the model including the non-linear term and \"chas\" variable (0.688 vs 0.687), but the AIC has stalled (at 3101) and BIC has started to increase (3122 vs 3118), suggesting that we are now adding complexity to our model without much explanatory benefit in return. The final nail in the coffin is that our newly added dummy variable is not statistically significant (see \"P>|t|\"). In a real world application it is unlikely that we would want to include this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm8 = sm.OLS(boston_IV,selected_DV8).fit()\n",
    "lm8.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the ANOVA method from above to compare all of the nested linear models that we have created - notice that as indicated before the models are ordered from least complex to most complex. We can see from this that our newest model fails to explain a statistically significant amount of additional variation. It's worth noting that all of the model diagnostic measures discussed here are somewhat conceptually related in terms of what they are tying to measure - it's not unexpected that they point in similar direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.anova_lm(lm,lm6,lm7,lm8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now completed the workshop portion or this module and should move on to the lab."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
